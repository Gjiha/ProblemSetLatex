\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{geometry}
\usepackage{algpseudocode}
\usepackage{amsmath}

\title{Problem Set}
\author{Francesco Cosciotti, franci2002.fc@gmail.com, 0323545; 
	\\Francesco Mongelli, francesco.mongelli03@libero.it, 0327829;
	\\ Alessio Fortini, fortinialessio1@gmail.com, 0324425}
\date{December 2023}

\begin{document}
	\maketitle
	
\section{Problema 1}
\subsection{Risoluzione del problema per $O(n^2)$}

Per dimostrare che $O(n^2)$ scambi siano sempre sufficienti, abbiamo pensato a un algoritmo piuttosto banale che, con un costo quadratico, riordini tutte le pinte.

Innanzitutto, abbiamo visto il bancone con le birre come un array $L[1:n]$ di record t.c. $Gen(L[k]) = \{M, F\}$ e $Beer(L[k]) = \{A, B\}$.

Vedere l'array in questo modo ci ha fatto capire abbastanza velocemente che le operazioni da effettuare per "riordinare" l'array (per riordinare si intende l'accoppiare ogni birra con il proprietario giusto) erano tre:
\begin{enumerate}
	\item essere in grado di capire se un'accoppiata fosse giusta o sbagliata;
	\item ricercare un'eventuale accoppiata giusta;
	\item essere in grado attraverso degli scambi contigui di accoppiarla con il proprietario del giusto genere.
\end{enumerate}

\subsubsection{Esecuzione dell'algoritmo}

Tutte queste operazioni possono essere svolte attraverso due cicli for annidati:
\begin{enumerate}
	\item Allo scorrere di un indice $i$ (che va da $1$ a $n$) verranno effettuati dei controlli sulle coppie; se l'accoppiata sarà sbagliata, farà partire i cicli successivi, altrimenti, scorrerà come se niente fosse;
	\item Questo ciclo viene fatto partire nel momento in cui $i$ incontra un'accoppiata sbagliata. In questo caso, si fa partire un indice $j$ (che va da $i$ a $n$) che scorre fintanto che non trova una birra che vada bene per la persona che si trova in $L[i]$;
	\item Una volta trovata la birra giusta, $j$ comincia ad effettuare degli scambi a ritroso in modo da portare la birra nella giusta posizione.
\end{enumerate}
	
	\subsubsection{Pseudocodice}
	\begin{algorithmic}
		\Function{Beer\_change}{$L[1:n]$}
			\For{$i \gets 1$ to $n$}:  \Comment{O(n)}
				\If{$A[i]$ è una coppia sbagliata}:
					\State $j \gets i$;
					\While{$Beer(L[j])$ non è giusta per $Gen(L[i])$}: \Comment{O(n)}
						\State incrementa $j$;
					\EndWhile
					\While{$j > i$}: \Comment{O(n)}
						\State Scambia $Beer(L[j])$ con $Beer(L[j-1])$ 
						\State decrementa $j$;
					\EndWhile
				\EndIf
			\EndFor
		\EndFunction
	\end{algorithmic}
	
	\subsubsection{Calcolo della complessità}
	Possiamo notare che questo algoritmo, nel caso peggiore, esegua un $O(n \times (n+n)) \rightarrow O(n \times 2n) \rightarrow O(2n^2) \rightarrow O(n^2)$.
\subsection{Dimostrazione che nel caso peggiore non si possano fare meno di $\Omega(n^2)$}

	\subsubsection{Istanza}
	Consideriamo un insieme di $n$ persone, divise equamente in $n/2$ maschi (M) e $n/2$ femmine (F). Supponiamo di avere due tipi di birre, A e B, e ogni maschio deve ricevere una birra di tipo A, mentre ogni femmina deve ricevere una birra di tipo B. Le birre sono disposte come segue:
	
	\[
	\begin{array}{cccccccccc}
		M & M & \ldots & M & | & F & F & \ldots & F \\
		A & A & \ldots & A & | & B & B & \ldots & B \\
	\end{array}
	\]
	\subsubsection{Dimostrazione della complessità}
	Le birre di tipo A sono posizionate a partire dalla $(n/2 + 1)$-esima posizione. Ogni successiva birra di tipo A si trova a una distanza di $n/2$ posizioni dalla birra da sostituire. Quindi, per ogni maschio, che è ordinato da 1 a $n/2$, sarà necessario effettuare $n/2$ scambi. Poiché il numero totale di maschi è $n/2$, il numero totale di scambi sarà $\Omega(n^2)$.
	



\subsection{Algoritmo con complessità O(n+k)}
\subsubsection{Proprietà}
Nel nostro studio del problema, abbiamo identificato una proprietà chiave legata agli scambi contigui che si è rivelata molto utile per minimizzare il numero di operazioni. Supponiamo di voler scambiare due valori in un array, uno situato all'inizio (chiamato \(A\)) e uno alla fine (chiamato \(B\)), con \(n\) elementi tra di loro (rappresentati come \(x_n\)). L'array iniziale è così strutturato:

\[
\begin{array}{ccccc}
	A & | & x_n & | & B
\end{array}
\]

Abbiamo osservato che spostare \(A\) nella posizione di \(B\) provoca uno "shifting" delle posizioni a sinistra di tutti gli elementi scambiati con \(A\). Ciò porta all'evoluzione dell'array come segue:

\[
\begin{array}{ccccc}
	x_n & | & B & | & A
\end{array}
\] 

Successivamente, al fine di ripristinare l'ordine originale, è sufficiente far risalire \(B\) in prima posizione. Ciò comporta uno "shifting" delle posizioni a destra di tutti gli elementi scambiati con \(B\), portando all'array finale:

\[
\begin{array}{ccccc}
	B & | & x_n & | & A
\end{array}
\] 

In altre parole, l'array risulta identico a quello iniziale, con l'unica differenza che \(A\) e \(B\) sono scambiati di posizione.
\subsubsection{Dimostrazione della proprietà}
Per dimostrare questa proprietà possiamo utilizzare un'induzione (da 1 a n) sul numero di elementi compresi fra la prima ed ultima posizione:
\begin{itemize}
	\item per n = 1 sappiamo che la proprietà è verificata in quanto l'array è identico a quello dell'esempio;
	\item per n = 2 avremo un array del tipo:
	\[
	\begin{array}{cccc}
		A & x_1 & x_2 & B
	\end{array}
	\] 
	portando A in ultima posizione eseguendo gli scambi eseguiremo delle operazioni del tipo:
	\[
	\begin{array}{ccccccc}
		| & A &\rightarrow& x_1 & | & x_2 & B
	\end{array}
	\]
	\[
	\begin{array}{ccccccc}
		x_1 & | & A & \rightarrow & x_2 & | & B
	\end{array}
	\]
	\[
	\begin{array}{ccccccc}
		x_1 & x_2 & | & A & \rightarrow & B & |
	\end{array}
	\]
	\[
	\begin{array}{cccc}
		x_1 & x_2 & B & A 
	\end{array}
	\]
	una volta in questa posizione facciamo risalire B in questo modo:
	\[
	\begin{array}{ccccccc}
		x_1 & | & x_2 & \leftarrow & B & | & A
	\end{array}
	\]
	\[
	\begin{array}{ccccccc}
		| & x_1 & \leftarrow & B & | & x_2 & A
	\end{array}
	\]
	\[
	\begin{array}{cccc}
		B & x_1 & x_2 & A
	\end{array}
	\]
	\item Assumendo sia vero per ogni n dimostriamolo per $n+1$
	\item Essendo n = n+1 l'array avrà una forma del tipo:
	\[
	\begin{array}{ccccccc}
		A & x_1 & \dots & x_n & x_{n+1} & B
	\end{array}
	\]
	contando $x_1 \dots x_2$ come un unico elemento eseguiremo delle operazioni sull'array del tipo:
	\[
	\begin{array}{cccccccccc}
		| & A & \rightarrow & x_1 & \dots & x_n & | & x_{n+1} & B
	\end{array}
	\]
	\[
	\begin{array}{cccccccccc}
		x_1 & \dots & x_n & | & A & \rightarrow & x_{n+1} & | & B
	\end{array}
	\]
	\[
	\begin{array}{cccccccccc}
		x_1 & \dots & x_n & x_{n+1} & | & A & \rightarrow & B & |
	\end{array}
	\]
	\[
	\begin{array}{ccccccc}
		x_1 & \dots & x_n & x_{n+1} & B & A
	\end{array}
	\]
	arrivati ad avere questa posizione eseguiremo le operazioni necessarie a far risalire B:
	\[
	\begin{array}{cccccccccc}
		x_1 & \dots & x_n & | & x_{n+1} & \leftarrow & B &| & A 
	\end{array}
	\]
	\[
	\begin{array}{cccccccccc}
		 | & x_1 & \dots & x_n & \leftarrow & B &| & x_{n+1} & A 
	\end{array}
	\]
	\[
	\begin{array}{ccccccc}
		B & x_1 & \dots & x_n & x_{n+1} & A
	\end{array}
	\]
	avendo ora un array di questo tipo possiamo avvermare di poter ritenere vera questa proprietà avendola verificata per $\forall n > 0$;
\end{itemize}
\subsubsection{Idea alla base di un algoritmo attorno a questa proprietà}
Dopo aver dimostrato la proprietà che semplifica gli scambi tra coppie di elementi adiacenti all'interno di un array, abbiamo deciso di applicarla al nostro problema specifico: ordinare le coppie sbagliate di genere diverso, come \{M, A\} con \{F, B\}. Questa strategia ci consente di ordinare due pinte dopo solo due passaggi attraverso l'array, portando a una complessità di tipo \(O(k \times n)\), dove \(k\) è il numero di coppie da correggere.

Successivamente, abbiamo creato un array ausiliario \(P[1:k]\), dove \(k\) è il numero di coppie da correggere. In questo array, abbiamo inserito le coppie maschio-femmina con pinte sbagliate, organizzandole in modo che i maschi occupino posizioni con indici dispari e le femmine occupino posizioni con indici pari. Questo processo di creazione di \(P\) richiede un tempo \(O(n)\).

Una volta creato \(P\), abbiamo riassegnato le posizioni delle coppie in modo che l'indice minore della coppia fosse in una posizione dispari e l'indice maggiore in una posizione pari.

Infine, abbiamo ciclato attraverso la lunghezza di \(P\) (operazione \(O(k)\)) per determinare quali coppie scambiare, sfruttando la proprietà dimostrata in precedenza.

Questo approccio ci consente di correggere le coppie sbagliate in modo efficiente e sistemico.

\subsubsection{Creazione dell'algoritmo}
Avendo fisate tutte queste idee ora bisogna soltanto applicarle nel modo giusto infatti:
\begin{enumerate}
	\item Scorriamo l'array $L$ al fine di individuare le birre mal assegnate, salvandole nell'array ausiliario $P$ e mantenendo invariate le proprietà degli indici. (Complessità: $O(n)$)
	
	\item Successivamente, scorriamo l'array $P$ riordinando le posizioni delle coppie in modo che il valore minore preceda sempre quello maggiore. (Complessità: $O(k)$)
	
	\item Ora scorriamo $P$ a coppie, salvando gli indici delle coppie in posizioni dispari e pari rispettivamente. (Complessità: $O(k)$)
	
	\item Dopo aver memorizzato gli indici, applichiamo la proprietà dimostrata in precedenza utilizzando due cicli per effettuare gli scambi necessari. (Complessità: $O(n+n)$)
\end{enumerate}

\subsubsection{pseudocodice}
\begin{algorithmic}
	\Function{BeerChange}{L[1:n]}
		Inizializza P[1:n] con tutti zero
		\State $t \gets 1$ 
		\State $m \gets 1$ 
		\State $f \gets 2$ 
		
		\For{$t < n$}: \Comment{O(n)}
			\If{se Gen(L[t]) è $M$ e ha una birra sbagliata}:
				\State $ P[m] \gets t$
				\State Incremento $m$ di due; \Comment{m è sempre dispari}
			\ElsIf{se Gen(L[t]) è $F$ e ha una birra sbagliata}:
				\State $P[f]\gets t$
				\State Incremento $f$ di due; \Comment{f è sempre pari}
			\EndIf
		\EndFor
		
		\State $w \gets 1$
		\While{$P[w] \ne 0$ } \Comment{O(k)}
			\If{$P[w+1] < P[w]$}
				\State Scambia;
			\EndIf
			\State Incremento k di due \Comment{vado a vedere solo le coppie}
		\EndWhile
		
		\State $j \gets 1$
		\While{$P[j] \ne 0$}: \Comment{O($k \over 2$)}
			\State $u \gets P[j]$
			\State $v \gets P[j+1] - 1$ 
			\While{$u < P[j]$}: \Comment{O(n)}
				\State Scambia Beer(L[u]) con Beer(L[u+1]);
				\State Incremento u;
			\EndWhile
			\While{$v > P[j]$}: \Comment{O(n)}
				\State Scambia Beer(L[v]) con Beer(L[v-1]);
				\State Decremento v;
			\EndWhile
			\State Incremento j di due;
		\EndWhile  
	\EndFunction
\end{algorithmic}

\subsubsection{Calcolo della coplessità}
Nel caso medio, quando $k < n$, l'algoritmo ha una complessità di tipo $O(n+k)$. Ciò è dovuto al fatto che la creazione di $P$ ha un costo di $O(n)$, il riordinamento di $P$ ha un costo di $O(k)$, e il terzo ciclo ha una complessità di $O\left(\frac{k}{2} \times 2n\right)$, ovvero $O(k \times n)$. Pertanto, la complessità totale dell'algoritmo è $O(n+k+k \times n)$, che può essere semplificata a $O(n+k \times n)$.

Nel caso peggiore, quando $k = n$, la complessità dell'algoritmo diventa $O(n^2)$, poiché il terzo ciclo avrà una complessità di $O(n^2)$ al posto di $O(k \times n)$.

In sintesi, l'algoritmo presenta una complessità che dipende dalla relazione tra $n$ e $k$, e nel caso medio risulta più efficiente rispetto al caso peggiore.

 
 	
	

\section{Problema 2}
La prima intuizione è stata di rappresentare la griglia $n \times m$ come un array $A[1:n]$, dove ogni posizione $A[i]$ ($\forall i \in [1,n]$) indica l'altezza della colonna. Quando si posiziona un blocco, è necessario scorrere due volte l'array da $x$ a $z$ (dove $z$ è l'angolo destro del blocco, calcolato come $x + w$): una per trovare l'altezza massima, chiamata $k$, e una seconda per sostituire i valori con $k + h$. Successivamente, si controlla se $k + h$ supera $m$. In tal caso, l'algoritmo restituisce l'indice della mossa in cui è stato posizionato l'ultimo blocco; altrimenti, continua a scorrere le mosse.

È evidente che questo approccio ha una complessità asintotica di $O(Nn)$ a causa della crescita lineare della lunghezza del blocco, mentre l'obiettivo è raggiungere una complessità di $O(N\sqrt{n})$.

\subsection{Idea alla base dell'algoritmo finale}

Con il suggerimento del professore Luciano Gualà, abbiamo capito che per ottenere una lunghezza lineare di $\sqrt{n}$, dovevamo suddividere l'array $A$ in $\sqrt{n}$ sezioni, chiamate "chunk", ciascuna di lunghezza $\sqrt{n}$. Questa partizione ci consente di salvare l'altezza massima di ogni chunk in un array $B[1:\sqrt{n}]$, dove ogni posizione $B[k]$ ($\forall k \in [1,\sqrt{n}]$) rappresenta l'altezza massima all'interno del $k$-esimo chunk.

Utilizzando l'array $B$, possiamo notare che la ricerca del massimo può essere eseguita in modo più efficiente, dividendo il blocco in tre parti:

\begin{enumerate}
	\item La prima sezione va dall'indice $x$ fino alla fine del chunk in cui si trova (chiamato $\alpha$), calcolato come $\bigl\lfloor { x\over \sqrt{n} }\bigr\rfloor$;
	\item La seconda va dall'inizio del chunk successivo a quello di $x$ fino al chunk immediatamente prima di quello in cui si trova $z$;
	\item La terza va dall'inizio del chunk in cui si trova $z$ (chiamato $\omega$), calcolato come $\bigl\lfloor { z \over \sqrt{n} }\bigr\rfloor$, fino alla posizione effettiva di $z$.
\end{enumerate}

Questa divisione consente di cercare il massimo in $A[x:(\alpha + 1) \sqrt{n} - 1]$, in $B[\alpha + 1:\omega - 1]$, e infine in $A[\omega \sqrt{n}:z]$. La ricerca del massimo $k$ può quindi essere eseguita in $O(\sqrt{n})$, poiché le lunghezze delle prime e terze sezioni sono al massimo $\sqrt{n}$, così come lo è $B$. Successivamente, sostituiremo i valori in $A[x:(\alpha + 1) \sqrt{n} - 1]$, $B[\alpha + 1:\omega - 1]$, e $A[\omega \sqrt{n}:z]$, assicurandoci di sostituire i valori in $B$ se $k + h$ supera i valori attualmente presenti in $B$. Grazie a questi scorrimenti, garantiamo che le liste $A$ e $B$ siano sempre aggiornate al momento in cui viene posizionato un blocco, portando la complessità dell'algoritmo a $O(N\sqrt{n})$.

\subsection{Specifiche riguardo l'algoritmo finale}

L'algoritmo progettato potrebbe essere sufficiente, ma abbiamo voluto aggiungere ulteriori casistiche e un array ausiliario aggiuntivo per velocizzare ulteriormente la ricerca del massimo e la sua sostituzione.

Innanzitutto, abbiamo introdotto un secondo array ausiliario chiamato $C[1:\sqrt{n}]$, in cui ogni posizione $C[j]$ ($\forall j \in [1:\sqrt{n}]$) indica tramite un valore booleano se il $j$-esimo chunk è spianato o meno (True se l'altezza di tutte le colonne appartenenti al chunk è uguale, False altrimenti).

Le diverse casistiche sono le seguenti:
\begin{itemize}
	\item Il blocco occupa un intero chunk: la ricerca del massimo è istantanea grazie a $B$, e i valori vengono sostituiti interamente in $A$ attraverso un ciclo, con un costo di $O(\sqrt{n})$. Successivamente, il valore di $C$ relativo al chunk viene impostato su True.
	
	\item Il blocco cade in un unico chunk ma non lo occupa interamente: si cerca il massimo in $A[x:z]$ (essendo $z - x < \sqrt{n}$ per costruzione), e poi si sostituiscono i valori ciclando in questo intervallo. Bisogna fare attenzione a sostituire il valore relativo al chunk in $B$ e a negare quello in $C$ se necessario.
	
	\item Il blocco occupa più di un chunk: questo caso è gestito come spiegato nella sezione precedente, con l'aggiunta che bisogna impostare il valore relativo ai chunk intermedi in $C$ e negare quelli relativi al chunk iniziale e finale.
\end{itemize}


\subsection{Pseudocodice}
\begin{algorithmic}
	\Function{GameOverChecker}{$N, n, m$}
		\State Inizializza $B[1:\sqrt{n}]$ con tutti zero 
		\State Inizializza $C[1:\sqrt{n}]$ con tutti True 
		\State $i \gets 1$ \Comment{Variabile per il conteggio delle mosse}
	
		\For{$i$ to $N$} \Comment{O($N$)}
			\State $x \gets x_i$
			\State $z \gets x_i + w_i$
			\State $Chunk_i \gets \lfloor \frac{x}{\sqrt{n}} \rfloor$
			\State $Chunk_e \gets \lfloor \frac{z}{\sqrt{n}} \rfloor$
			\State $k \gets 0$
	
			\If{$Chunk_i = Chunk_e$ and $w_i = \sqrt{n}$}
				\State $k \gets B[Chunk_i] + h_i$
				\For{$t \gets x$ to $z$} \Comment{O($\sqrt{n}$)}
				\State $A[t] \gets k$
				\EndFor
			\State $C[Chunk_i] \gets True$
			\EndIf
	
			\If{$Chunk_i = Chunk_e$} \Comment{da  vedere come se fosse un else if}
				\State $k \gets \max(A[x:z]) + h_i$ \Comment{O($\sqrt{n}$)}
				\For{$j \gets x$ to $z$} \Comment{O($\sqrt{n}$)}
					\State $A[j] \gets k$
				\EndFor
				\If{$B[Chunk_i] < k$}
					\State $B[Chunk_i] \gets k$
				\EndIf
				\State $C[Chunk_i] \gets False$
			\Else
				\If{$C[Chunk_i] = True$}
					\State $k \gets B[Chunk_i]$
				\Else
					\State $k \gets \max(A[x : ((Chunk_i +1) \times \sqrt{n}) - 1])$ \Comment{O($\sqrt{n}$)}
				\EndIf
				
				\If{$k < \max(B[Chunk_i + 1 : Chunk_e -1]) $}
					\State $k \gets \max(B[Chunk_i + 1 : Chunk_e -1])$ \Comment{O($\sqrt{n}$)}
				\EndIf				
				
				\If{$C[Chunk_e] = True$ and $k < B[Chunk_e]$}
					\State $k \gets B[Chunk_e] $
				\Else
					\State $k \gets \max(A[Chunk_e \times \sqrt{n} : z])$  \Comment{O($\sqrt{n}$)}
				\EndIf
				
				\State $k \gets k +h_i$
				
				\State $ \alpha \gets x$
				\For{$\alpha$ to $((Chunk_i +1) \times \sqrt{n}) - 1$ } \Comment{O($\sqrt{n}$)}
					\State $A[\alpha] \gets k$
				\EndFor
				\If{$k > B[Chunk_i]$}
					\State $B[Chunk_i] \gets k$
				\EndIf
				\State $ \beta \gets Chunk_i + 1$
				\For{$\beta$ to $ Chunk_e - 1$} \Comment{O($\sqrt{n}$)}
					\State $B[\beta] \gets k$
				\EndFor
				\State $ \gamma \gets Chunk_e \times \sqrt{n}$
				\For{$\gamma$ to $z$} \Comment{O($\sqrt{n}$)}
					\State $A[\gamma] \gets k$
				\EndFor
				\If{$k > B[Chunk_e]$}
					\State $B[Chunk_e] \gets k$
				\EndIf
				
				\State $C[Chunk_i] \gets False$
				\State $C[Chunk_e] \gets False$
			\EndIf
	
			\If {$k > m$}
				\Return l
			\EndIf
		
		\EndFor
		\\\Return Il gioco continua
	
	\EndFunction
\end{algorithmic}

\section{Problema 3}
\subsection{Distance}
\subsubsection{Costruzione struttura dati}
Affinché la nostra query possa mantenere un costo temporale O(1), dovremo avere accesso preventivo a qual è la lunghezza di ciascun nodo. Ciò, in realtà, è abbastanza semplice: ci basterà costruire, dato un albero T di n nodi, un array indicizzato di n celle, dove ogni cella contiene:
\begin{itemize}
	\item Un nodo.
	\item Il peso associato al nodo, inteso come la somma degli archi sul cammino, d'ora in poi indicato come weight(v) dato un generico nodo v.
\end{itemize}
Agli scopi di questa spiegazione, l'ordinamento dell'array è irrilevante: presupporremo di avere già un riferimento esplicito alla loro posizione.
\subsubsection{Query}
Una volta palesata la struttura dati, è relativamente semplice intuire come calcolare in tempo 0(1) la lunghezza tra due nodi. Risulterà, infatti: \newline
\newline

\begin{algorithmic}
	\Function{dist}{$u, v$} 
	\State\Return $weight(v) - weight(u)$
	\EndFunction
\end{algorithmic}
\subsection{Level Ancestor}
\subsubsection{Considerazioni iniziali e algoritmo banale}
Posto inizialmente il problema di rintracciare un antenato/figlio in base ad un qualsivoglia criterio, la risposta banale è ovvia: scorriamo la nostra struttura dati fino a che non troviamo il nodo bersaglio.
Questa soluzione, sebbene corretta, non rispetta la necessità di creare una query che restituisca in tempo $O(\log(n))$ la risposta alla nostra domanda, attenendosi, nel caso peggiore, ad un O(n).
Per ottenere, dunque, il tempo desiderato, dovremo avere dei riferimenti espliciti già in fase di creazione della struttura dati.
\subsubsection{Costruzione struttura dati}
Per guadagnare una maggiore efficienza in fase di ricerca, dato un qualsiasi albero T di n nodi, struttureremo la base di dati in due parti fondamentali.
\begin{itemize}
	\item Un array indicizzato di lunghezza $n$, t.c. ogni cella contenga un nodo dell'albero. L'ordinamento, per gli scopi di questa spiegazione, è irrilevante: si presupporrà che, al momento della query, si sappia già dove è locato il nodo di partenza v.
	\item Per ogni cella dell'array, successivamente, verrà definita un "vettore degli antenati", ossia un vettore di puntatori, t.c. ciascuno indichi, se esiste, l'antenato lontano $2^i$ archi
	dal nodo. 
	Avremo dunque, per un qualsiasi nodo u, un riferimento esplicito all'antenato in posizione 0 (se stesso), 1, 2, 4, 8, ecc.
\end{itemize}

Per calcolare la dimensione totale della struttura, ci basta osservare che ogni "vettore degli antenati" consisterà di \textit{i} elementi, per il massimo \textit{i} t.c. $2^i$ sia al più \textit{n}. Poiché, salvando solo gli antenati lontani potenze di 2 archi, e considerando la rappresentazione binaria di n, tali antenati saranno al più il numero di cifre necessarie a rappresentare n in binario (visto che salvo solo gli antenati del tipo $1_2$, $10_2$, $100_2$, ecc.), quindi ogni vettore avrà dimensione $O(\log(n))$. Ripetendo il processo per ogni nodo nell'array di lunghezza n, la dimensione finale sarà $O(n\log(n))$.

\subsubsection{Query}
La fase di query sfrutterà la strutturazione della base dati per ritornare in tempo $O(\log(n))$ il nostro nodo bersaglio. 
LA(v,k) sarà una chiamata ricorsiva, che consterà di 3 esiti possibili:
\begin{itemize}
	\item L'antenato a livello k non esiste. Caso di fallimento: restituiremo semplicemente NULL. Facilmente strutturabile con un qualche tipo di check all'inizio del codice.
	\item L'antenato a livello k esiste, e k è della forma $2^i$ per un qualche intero i. Caso base: ritorneremo il nodo associato al puntatore corrispondente nel vettore degli antenati.
	\item L'antenato a livello k esiste, ma k non è della forma $2^i$ per un qualche intero i. Caso ricorsivo: ritorneremo una nuova chiamata LA(v, $k_i$), con $k_i = k - q$ con q la più grande potenza di 2 minore di k.
\end{itemize}
\subsubsection{Pseudocodice}
LA(k,v)\newline
Sia T un albero di n nodi
\begin{algorithmic}
	\If{v è la radice di T \textbf{and} $k > 0$} //Se non esiste
	\State \textbf{return} NULL
	\Else   
	\State $q = \lfloor\log_2{k}\rfloor$
	\State $u \gets$ nodo del vettore degli antenati in posizione $q$
	\If{$q = k$}
	\State \textbf{return} $u$
	\Else
	\State \textbf{return} LA($u, k-q$)
	\EndIf
	\EndIf
\end{algorithmic}
\subsubsection{Costo}
Affinché si possa analizzare il costo dell'algoritmo di query, è bene scomporne nuovamente il funzionamento in base ai possibili esiti:
\begin{itemize}
	\item Nel caso in cui l'antenato esista e risulti $k = 2^i$ per un qualche intero i, il costo temporale dell'algoritmo sarà $O(1)$.
	\item Nel caso in cui l'antenato esista e risulti che k non sia potenza di 2, si avvierà una serie di chiamate ricorsive, di cui possiamo studiare il costo temporale facendo queste considerazioni: ogni chiamata ha costo $O(1)$; ogni numero binario può essere scomposto in somme di numeri binari che rappresentano potenze di 2 (ad esempio: $1001101_2$ può essere scomposo in $1000000_2 + 1000_2 + 100_2 +1_2$). Quindi per ogni k che non è potenza di 2 la funzione verrà richiamata al più per ogni "1" presente nella rappresentazione binaria di k, che è al più il numero di cifre necessario per rappresentare k in binario, ovvero $O(\log(k))$, ma, visto che in questo caso $k = O(n)$ (poiché l'antenato esiste), il costo temporale sarà $O(\log(n))$.
\end{itemize}

\subsection{Weighted Level Ancestor}
\subsubsection{Costruzione struttura dati}
Per questo punto del terzo problema abbiamo pensato, sostanzialmente, ad una combinazione dei precedenti due punti. Infatti, per ogni nodo v dell'albero T salveremo:
\begin{itemize}
	\item Il suo peso dalla radice (inteso come dist(r, v), con r radice di T) che quindi chiameremo weight(v), come informazione satellite all'interno del nodo stesso.
	\item Un vettore indicizzato di antenati (che chiamo $v.antenati$), che segue lo stesso criterio del secondo punto del problema (contiene solo gli antenati distanti potenze di 2).
\end{itemize}
In questo modo la struttura dati avrà complessita spaziale $O(n logn)$ poiché (come nei punti precedenti) per ogni nodo salvo una informazione satellite e un vettore di antenati di grandezza $O(logn)$ (dimostrato nel punto precedente).
Agli scopi di questa spiegazione, l'ordinamento dell'array è irrilevante: presupporremo di avere già un riferimento esplicito alla loro posizione.
\subsubsection{Query}
La fase di risposta alla query segue la falsa riga del secondo punto: $WLA(v, \Delta)$ sarà una chiamata ricorsiva, che:
\begin{itemize}
	\item Elaborerà l'antenato $u$ meno profondo di $v$ nel suo vettore degli antenati tale che $dist(u, v) < \Delta$.
	\item Richiamerà la funzione $WLA(u,  \Lambda)$, con $u$ come definito prima e $\Lambda = \Delta - dist(u,v)$.
\end{itemize}
Il caso base da verificare prima di richiamare la funzione si verifica se il nodo $u$ è il nodo $v$, in tal caso la funzione ritornerà il padre di $v$. Poiché so che $v$ è il suo antenato meno profondo tale che $dist(u, v) < \Delta$, e al contempo ho verificato se suo padre rispetta la proprietà richiesta, essendo un suo antenato distante $1 (= 2^0)$ e ho scoperto che $dist(padre(v), v) \ge \Delta$.


\subsubsection{Pseudocodice}
$WLA(v, \Delta)$\newline
Sia T un albero di n nodi
\begin{algorithmic}
	\If{v è la radice di T \textbf{and} $\Delta > 0$} //Se non esiste
	\State \textbf{return} NULL
	\EndIf
	\State $i = 0$
	\While{$dist(v.antenati[i], v) < \Delta$}
	\State $i \mathrel{+}= 1$
	\EndWhile
	\State $u \gets v.antenati[i]$
	\If{$v = u$}
	\State \textbf{return} $padre(v)$
	\Else
	\State $\Lambda$ = $\Delta - dist(u, v)$
	\State \textbf{return} $WLA(u, \Lambda)$
	\EndIf
\end{algorithmic}
\subsubsection{Costo}
Per studiare la complessità temporale dell'algoritmo basta fare un parallelismo con i punti precedenti: 
\begin{itemize}
	\item Ogni operazione di lettura del vettore degli antenati e ogni chiamata della funzione $dist(u, v)$ (la funzione creata nel primo punto) costa $O(1)$, infine il ciclo $while$ gira al più $O(\log n)$.
	\item Per motivi paralleli a quelli del secondo punto al massimo la funzione verrà richiamata $O(\log n)$ volte.
\end{itemize}
\textbf{\textit{Quindi, in totale, l'algoritmo avrà complessita temporale $O(\log n \log n)$, che non è la specifica richiesta dal problema, non siamo riusciti a risolvere il terzo punto causa tempistiche e impegni, ma ci abbiamo provato e abbiamo comunque voluto presentare la miglior risoluzione trovata dal gruppo, anche se sbagliata o incompleta.}}
\end{document}